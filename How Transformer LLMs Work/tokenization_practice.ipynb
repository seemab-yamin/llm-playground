{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting transformers\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a9/b6/5257d04ae327b44db31f15cce39e6020cc986333c715660b1315a9724d82/transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "     ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/10.4 MB 4.3 MB/s eta 0:00:03\n",
      "      --------------------------------------- 0.2/10.4 MB 3.1 MB/s eta 0:00:04\n",
      "      --------------------------------------- 0.2/10.4 MB 2.0 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.4/10.4 MB 2.0 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.4/10.4 MB 1.8 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.6/10.4 MB 2.2 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 0.8/10.4 MB 2.5 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/10.4 MB 2.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.1/10.4 MB 2.8 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.4/10.4 MB 3.0 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.5/10.4 MB 3.0 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 1.8/10.4 MB 3.3 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.0/10.4 MB 3.2 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.2/10.4 MB 3.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 2.4/10.4 MB 3.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 2.8/10.4 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 2.9/10.4 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.0/10.4 MB 3.6 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.2/10.4 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.3/10.4 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.3/10.4 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 3.5/10.4 MB 3.4 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 3.6/10.4 MB 3.4 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 3.7/10.4 MB 3.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 3.9/10.4 MB 3.4 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.1/10.4 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 4.2/10.4 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 4.4/10.4 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 4.5/10.4 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 4.5/10.4 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 4.6/10.4 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 4.8/10.4 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 4.8/10.4 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 4.9/10.4 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 5.0/10.4 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 5.1/10.4 MB 3.0 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 5.3/10.4 MB 3.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 5.4/10.4 MB 3.1 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.5/10.4 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.5/10.4 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.7/10.4 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 5.8/10.4 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 5.8/10.4 MB 3.0 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.0/10.4 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.0/10.4 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.2/10.4 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.2/10.4 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 6.4/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 6.5/10.4 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 6.6/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 6.6/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 6.8/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 6.8/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 7.0/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.1/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.3/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.3/10.4 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.5/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.5/10.4 MB 2.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 7.6/10.4 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 7.8/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.0/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 8.2/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 8.2/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.5/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 8.7/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.8/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.9/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.9/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.9/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.2/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.3/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.3/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.5/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 9.8/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 9.8/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.1/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.1/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.2/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.3/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.4/10.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.4/10.4 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/93/27/1fb384a841e9661faad1c31cbfa62864f59632e876df5d795234da51c395/huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "     ---------------------------------------- 0.0/481.4 kB ? eta -:--:--\n",
      "     --------------- ---------------------- 194.6/481.4 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------- ----------------- 256.0/481.4 kB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 337.9/481.4 kB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 399.4/481.4 kB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 399.4/481.4 kB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 399.4/481.4 kB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 481.4/481.4 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e6/b6/072a8e053ae600dcc2ac0da81a23548e3b523301a442a6ca900e92ac35be/tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.4 MB 3.2 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 0.1/2.4 MB 1.8 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.2/2.4 MB 1.5 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.2/2.4 MB 1.5 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.5/2.4 MB 2.3 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.6/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.8/2.4 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.9/2.4 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.0/2.4 MB 2.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.2/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.3/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.5/2.4 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.7/2.4 MB 2.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.8/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.9/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.1/2.4 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.4/2.4 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/e2/b011c38e5394c4c18fb5500778a55ec43ad6106126e74723ffaee246f56e/safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "     ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
      "     ----------- --------------------------- 92.2/308.9 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 308.9/308.9 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert-based-cased is a pre-trained encoder model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"google-bert/bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-cased\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process input sentence and extract token ids\n",
    "token_ids = tokenizer(sentence).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8667, 1291, 106, 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "Hello\n",
      "World\n",
      "!\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "# these token ids represents tokens\n",
    "# to get these tokens we need to decode token ids\n",
    "for ti in token_ids:\n",
    "    print(tokenizer.decode(ti))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLS -> classification token represents the entire input\n",
    "\n",
    "SEP -> token signifies the end of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use colors to differentiate tokens\n",
    "colors = [\"102;194;165\",\"252;141;98\", \"141;160;203\", \"231;138;195\", \"166;216;84\", \"255;217;47\"]\n",
    "\n",
    "def show_tokens(sentence: str, tokenizer_name: str):\n",
    "    \"\"\" Show the tokens, each separated by a different color \"\"\"\n",
    "\n",
    "    # Load the tokenizer and tokenize the input\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "\n",
    "    # Extract vocabulary length\n",
    "    print(f\"Vocab length: {len(tokenizer)}\")\n",
    "\n",
    "    # Print a colored list of tokens\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "ðŸŽµ é¸Ÿ\n",
    "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
    "12.0*50=600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 28996\n",
      "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mCA\u001b[0m \u001b[0;30;48;2;166;216;84m##PI\u001b[0m \u001b[0;30;48;2;255;217;47m##TA\u001b[0m \u001b[0;30;48;2;102;194;165m##L\u001b[0m \u001b[0;30;48;2;252;141;98m##I\u001b[0m \u001b[0;30;48;2;141;160;203m##Z\u001b[0m \u001b[0;30;48;2;231;138;195m##AT\u001b[0m \u001b[0;30;48;2;166;216;84m##ION\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mF\u001b[0m \u001b[0;30;48;2;102;194;165m##als\u001b[0m \u001b[0;30;48;2;252;141;98m##e\u001b[0m \u001b[0;30;48;2;141;160;203mNone\u001b[0m \u001b[0;30;48;2;231;138;195mel\u001b[0m \u001b[0;30;48;2;166;216;84m##if\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m>\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195melse\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47mtwo\u001b[0m \u001b[0;30;48;2;102;194;165mta\u001b[0m \u001b[0;30;48;2;252;141;98m##bs\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47mThree\u001b[0m \u001b[0;30;48;2;102;194;165mta\u001b[0m \u001b[0;30;48;2;252;141;98m##bs\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m12\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m*\u001b[0m \u001b[0;30;48;2;231;138;195m50\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m600\u001b[0m \u001b[0;30;48;2;102;194;165m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/google-bert/bert-base-cased\n",
    "show_tokens(text, \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAPITALIZATION is represented by many tokens. \"##\" in front of token characters represents that it is part of the previous token.\n",
    "UNK token represents an unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 100263\n",
      "\u001b[0;30;48;2;102;194;165m\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \u001b[0;30;48;2;231;138;195m ï¿½\u001b[0m \u001b[0;30;48;2;166;216;84mï¿½\u001b[0m \u001b[0;30;48;2;255;217;47mï¿½\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_tokens\u001b[0m \u001b[0;30;48;2;231;138;195m False\u001b[0m \u001b[0;30;48;2;166;216;84m None\u001b[0m \u001b[0;30;48;2;255;217;47m elif\u001b[0m \u001b[0;30;48;2;102;194;165m ==\u001b[0m \u001b[0;30;48;2;252;141;98m >=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m   \u001b[0m \u001b[0;30;48;2;141;160;203m \"\u001b[0m \u001b[0;30;48;2;231;138;195m Three\u001b[0m \u001b[0;30;48;2;166;216;84m tabs\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m \"\u001b[0m \u001b[0;30;48;2;252;141;98m      \u001b[0m \u001b[0;30;48;2;141;160;203m \"\n",
      "\u001b[0m \u001b[0;30;48;2;231;138;195m12\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/Xenova/gpt-4/\n",
    "# gpt-4 tokenizer is called tiktoken, and we can't access its model because it is a proprietary model\n",
    "show_tokens(text, \"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4 Tokenizer uses less tokens to represent input sentence. It doesn't include CLS token becuase this tokenizer is for generation purpose.\n",
    "\n",
    "There is a trade-off in choosing tokenizer. A tokenizer like GPT-4 with more vocabulary will need less tokens to represent a sentence but it will also need to calculate more embeddings for each tokens.\n",
    "\n",
    "More tokenizers can be found on Hugging face if want to taste other tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 200000\n",
      "\u001b[0;30;48;2;102;194;165m\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203m ï¿½\u001b[0m \u001b[0;30;48;2;231;138;195mï¿½\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
      "\u001b[0m \u001b[0;30;48;2;255;217;47mshow\u001b[0m \u001b[0;30;48;2;102;194;165m_tokens\u001b[0m \u001b[0;30;48;2;252;141;98m False\u001b[0m \u001b[0;30;48;2;141;160;203m None\u001b[0m \u001b[0;30;48;2;231;138;195m elif\u001b[0m \u001b[0;30;48;2;166;216;84m ==\u001b[0m \u001b[0;30;48;2;255;217;47m >=\u001b[0m \u001b[0;30;48;2;102;194;165m else\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m two\u001b[0m \u001b[0;30;48;2;231;138;195m tabs\u001b[0m \u001b[0;30;48;2;166;216;84m:\"\u001b[0m \u001b[0;30;48;2;255;217;47m   \u001b[0m \u001b[0;30;48;2;102;194;165m \"\u001b[0m \u001b[0;30;48;2;252;141;98m Three\u001b[0m \u001b[0;30;48;2;141;160;203m tabs\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m \"\u001b[0m \u001b[0;30;48;2;255;217;47m      \u001b[0m \u001b[0;30;48;2;102;194;165m \"\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98m12\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m50\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m600\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/Xenova/gpt-4o/\n",
    "show_tokens(text, \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
